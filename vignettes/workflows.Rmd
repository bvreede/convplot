---
title: "Basic workflow with talkr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{workflow}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8
)
```

```{r setup}
library(talkr)
```

## Loading the data

We will be using the IFADV corpus as example data for the workflow of `talkr`. This is a corpus consisting of 20 dyadic conversations in Dutch, published by the Nederlandse Taalunie in 2007 ([source](https://fon.hum.uva.nl/IFA-SpokenLanguageCorpora/IFADVcorpus/))

The snippet below initializes the talkr dataset using the ifadv data. For more information about the IFADV dataset, see the [repository link](https://github.com/elpaco-escience/ifadv).

```{r}
data <- init(ifadv::ifadv) 

```

Essential to any `talkr` workflow is a minimal set of data fields. These are the following: \* `source`: the source conversation (a corpus can consist of multiple sources) \* `begin`: begin time (in ms) of an utterance \* `end`: end time (in ms) of an utterance \* `utterance`: content of an utterance \* `participant`: the person who produced the utterance

The `init()` function takes these minimal fields and generates a few more based on them. These are: \* `uid`: a unique identifier at utterance-level, used to identify, select and felter specific utterances \* `duration`: the duration (in ms) of the utterance, generated by subtracting `begin` from `end`

The `init()` function can be used to rename columns if necessary. For example, if the column `participant` is named `speaker`, we can rename it as follows:

```r
talkr_data <- init(data,
             participant = "speaker")
```

A dataset can contain additional fields. For instance, the IFADV sample dataset also contain `language` (which is Dutch) and `utterance_raw` (a fuller, less processed version of the utterance content). It also contains measures related to turn-taking and timing, including `FTO` (floor transfer offset, the offset between current turn and that of a prior participant, in milliseconds) and `freq` and `rank`, frequency measures of the utterance content.

## Workflow 1: Quality control

### Summary statistics

The `report_stats` function provides a simple summary of a dataset, including the total number of utterances, the total duration of the conversation, the number of participants, and the number of sources.

```{r report_stats}
report_stats(data)
```

### Visual quality checks

The `plot_quality` function provides a visual check of the nature of the data, by visualizing the distribution of turn durations, and transition timing.

Transition timing is similar to FTO, but calculated without additional quality checks: transitions are identified when the participant changes from one turn to the next. The transition time is then calculated as the difference between the beginning of the turn of the new participant, and the end of the turn of the previous one.

By default, `plot_quality()` will plot the entire dataset:

```{r}
plot_quality(data)

```

Quality plots can also be run for a specific source:

```{r}
plot_quality(data, source = "/dutch2/DVA8K")
```

A quality plot consists of three separate visualizations, all designed to allow rapid visual inspection and spotting oddities:
1. A density plot of turn durations. This is normally expected to look like a distribution that has a peak around 2000ms (2 seconds) and maximum lengths that do not far exceed 10000ms (10 seconds) (Liesenfeld & Dingemanse 2022). The goal of this plot is to allow eyeballing of oddities like turns of extreme durations or sets of turns with the exact same duration (unlikely in carefully segmented conversational data).
2. A density plot of turn transition times. A plot like this is expected to look like a normal distribution centered around 0-200ms (Stivers et al. 2009). Deviations from this may signal problems in the dataset, for instance due to imprecise or automated annotation methods.
3. A scatterplot of turn transition (x) by turn duration. This combines both distributions and is expected to look like a cloud of datapoints that is thickest in the middle region. Any standout patterns (for instance, turns whose duration is equal to their transition time) are indicative of problems in the segmentation or timing data.


## Workflow 2: Plot conversations

Another key use of `talkr` is to visualize conversational patterns. A first way to do so is `geom_turn()`, a ggplot2-compatible geom that visualizes the timing and duration of turns in a conversation.

We can start by simply visualizing some of the conversations in the dataset. Here we sample the first four and plot the first minute of each. We display them together using `facet_wrap()` by `source`.

```{r geom_turn_demon1}
library(ggplot2)

# we simplify participant names
conv <- data |>
  dplyr::group_by(source) |>
  dplyr::mutate(participant = as.character(factor(participant, labels=c("A","B"))))

# select first four conversations
these_sources <- unique(data$source)[1:4]

conv |>
  dplyr::filter(end < 60000, # select first 60 seconds
                source %in% these_sources) |> # filter to keep only these conversations
  ggplot(aes(x = end, y = participant)) +
  geom_turn(aes(
    begin = begin,
    end = end)) +
  xlab("Time (ms)") +
  ylab("") +
  theme_turnPlot() +
  facet_wrap(~source) # facet to show the conversations side by side
```

More often, we will want to plot a single conversation and explore it in some more detail. Let's zoom in on one of these first four. If we plot it without further tweaking, it is not the most helpful: the conversation is 15 minutes long and it is hard to appreciate its structure when we put it all on a single line.

```{r geom_turn_demo_2}

conv |>
  dplyr::filter(source == "/dutch2/DVA12S") |>
  ggplot(aes(x = end, y = participant)) +
  geom_turn(aes(
    begin = begin,
    end = end)) +
  xlab("Time (ms)") +
  ylab("") +
  theme_turnPlot()

```

So what we do is cut up the conversation into lines using `add_lines()`. By default, this will cut the conversation into lines of 60000ms each (1 minute), creating as many lines as needed. For now, let's focus on the first 5 minutes, which we can do by filtering for `line_id < 6` after we've added lines.

```{r geom_turn_demo_3}

conv |>
  add_lines() |> # add lines
  dplyr::filter(source == "/dutch2/DVA12S",
                line_id < 6) |> # limit to the first five lines
  ggplot(aes(x = line_end, y = line_participant)) +
  geom_turn(aes(
    begin = line_begin, # the begin and end aesthetics are now line-relative
    end = line_end)) +
  scale_y_reverse(breaks = seq(1, max(conv_token$line_id))) +  
  xlab("Time (ms)") +
  ylab("") +
  theme_turnPlot()

p <- last_plot()

```

We can style a plot like this using any available variables For instance, let's add a `fill` that corresponds to `duration`:

```{r step9}

p +
  ggtitle("Turns coloured by duration") +
  geom_turn(aes(
    begin = line_begin,
    end = line_end,
    fill=duration
  )) +
  viridis::scale_fill_viridis(option="A",direction=-1)

```

Or we can highlight turns that are produced in overlap:

```{r step10}

p +
  ggtitle("Turns produced in overlap") +
  geom_turn(aes(
    begin = line_begin,
    end = line_end,
    fill=overlap,
    colour=overlap
  )) 


```
So far we have just visualized the temporal structure. But conversational turns typically consist of words and other elements. 

We can start looking into the internal structure of turns by plotting occurrence of tokens. 

To do so, we first need to calculate the token frequencies:

```{r}
conv_tokens <- tokenize(conv)

conv_tokens
```


With information about tokens in hand, we can start asking questions. For instance, what are words that are quite frequent and that appear in utterance-initial position?

```{r}

conv |>
  add_lines(line_duration=15000) |>
  dplyr::filter(source == "/dutch2/DVA12S",
                line_id < 4) |> # let's look at the first three lines
  ggplot(aes(x = line_end, y = line_participant)) +
  scale_y_reverse(breaks = seq(1, max(conv_token$line_id))) + # we reverse the axis because lines run top to bottom
  geom_turn(aes(
    begin = line_begin,
    end = line_end,
    fill = nwords)) +
  xlab("Time (ms)") +
  ylab("") +
  theme_turnPlot()

p + 
  geom_token(aes(data=conv_tokens |> filter(source == "/dutch2/DVA12S"),
    begin=line_begin,
    end = line_end))


```

### Notes & orphaned text

Token frequencies are calculated over the entire dataset. If you want source-specific data, you can filter the source prior to tokenization:

```{r}
tokens_DVA9M <- data |>
  dplyr::filter(source == "/dutch2/DVA9M") |>
  tokenize()

tokens_DVA9M
```
