---
title: "Basic workflow with talkr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{workflow}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8
)
```

```{r setup}
library(talkr)
```

## Loading the data

We will be using the IFADV corpus as example data for the workflow of `talkr`. This is a corpus consisting of 20 dyadic conversations in Dutch, published by the Nederlandse Taalunie in 2007 ([source](https://fon.hum.uva.nl/IFA-SpokenLanguageCorpora/IFADVcorpus/)). A prepared dataset can be downloaded by installing the `ifadv` package:

```{r install_data_package}
# install.packages("devtools")
devtools::install_github("elpaco-escience/ifadv")
```

We will initialize the talkr dataset using the ifadv data, as follows:

```{r}
data <- init(ifadv::ifadv)
```

Essential to any `talkr` workflow is a minimal set of data fields. These are the following: \* `source`: the source conversation (a corpus can consist of multiple sources) \* `begin`: begin time (in ms) of an utterance \* `end`: end time (in ms) of an utterance \* `utterance`: content of an utterance \* `participant`: the person who produced the utterance

The `init()` function takes these minimal fields and generates a few more based on them. These are: \* `uid`: a unique identifier at utterance-level, used to identify, select and felter specific utterances \* `duration`: the duration (in ms) of the utterance, generated by subtracting `begin` from `end`

The `init()` function can be used to rename columns if necessary. For example, if the column `participant` is named `speaker`, we can rename it as follows:

```{r init_demo}
talkr_data <- init(data,
             participant = "speaker")
```

A dataset can contain additional fields. For instance, the IFADV sample dataset also contain `language` (which is Dutch) and `utterance_raw` (a fuller, less processed version of the utterance content). It also contains measures related to turn-taking and timing, including `FTO` (floor transfer offset, the offset between current turn and that of a prior participant, in milliseconds) and `freq` and `rank`, frequency measures of the utterance content.

## Workflow 1: Quality control

### Summary statistics

The `report_stats` function provides a simple summary of a dataset, including the total number of utterances, the total duration of the conversation, the number of participants, and the number of sources.

```{r report_stats}
report_stats(data)
```

### Visual quality checks

The `plot_quality` function provides a visual check of the nature of the data, by visualizing the distribution of turn durations, and transition timing.

Transition timing is similar to FTO, but calculated without additional quality checks: transitions are identified when the participant changes from one turn to the next. The transition time is then calculated as the difference between the beginning of the turn of the new participant, and the end of the turn of the previous one.

By default, `plot_quality()` will plot the entire dataset:

```{r}
plot_quality(data)

```

Quality plots can also be run for a specific source:

```{r}
plot_quality(data, source = "/dutch2/DVA8K")
```

A quality plot consists of three separate visualizations, all designed to allow rapid visual inspection and spotting oddities:
1. A density plot of turn durations. This is normally expected to look like a distribution that has a peak around 2000ms (2 seconds) and maximum lengths that do not far exceed 10000ms (10 seconds) (Liesenfeld & Dingemanse 2022). The goal of this plot is to allow eyeballing of oddities like turns of extreme durations or sets of turns with the exact same duration (unlikely in carefully segmented conversational data).
2. A density plot of turn transition times. A plot like this is expected to look like a normal distribution centered around 0-200ms (Stivers et al. 2009). Deviations from this may signal problems in the dataset, for instance due to imprecise or automated annotation methods.
3. A scatterplot of turn transition (x) by turn duration. This combines both distributions and is expected to look like a cloud of datapoints that is thickest in the middle region. Any standout patterns (for instance, turns whose duration is equal to their transition time) are indicative of problems in the segmentation or timing data.


## Workflow 2: Plot conversations

Another key use of `talkr` is to visualize conversational patterns.

Individual conversations can be plotted quickly using `plot_turns_tokens()`. The default setting is to plot the first 60 seconds of the first source in the data, overlaying the 10 most frequent tokens.

```{r}
plot_turns_tokens(data)
```

We can set other defaults; e.g. a specific source, a different time window, and a different number of tokens:

```{r}
plot_turns_tokens(data, source = "/dutch2/DVA9M",
                  begin = 120,
                  duration = 120,
                  maxrank = 20)
```

For more control over the plot, two specific geometries are available: `geom_turn` and `geom_token`. In addition, there is a `talkr`-specific theme provided.

```{r}
library(ggplot2)

p <- data |>
  dplyr::filter(source == "/dutch2/DVA9M") |>
  dplyr::filter(end < 60000) |>
  ggplot(aes(x = end, y = participant)) +
  geom_turn(aes(
    begin = begin,
    end = end)) +
  xlab("Time (ms)") +
  ylab("") +
  theme_turnPlot()

p
```

This plot can be overlayed with plotted occurrences of tokens.

To do so, we first need to calculate the token frequencies:

```{r}
tokens <- tokenize(data)

tokens
```

Token frequencies are calculated over the entire dataset. For source-specific data, it is recommended to filter the source prior to tokenization:

```{r}
tokens <- data |>
  dplyr::filter(source == "/dutch2/DVA9M") |>
  tokenize()

tokens
```

Before we plot the tokens over the turns, we need to select the tokens we want to plot (e.g. the top 10 ranked), and the time window they occur in:

```{r}
tokenselection <- tokens |>
  dplyr::filter(relative_time < 60000) |>
  dplyr::filter(rank <= 10)
```

We can plot the tokens over the turns.

```{r}
p +
geom_token(data = tokenselection,
           aes(x = relative_time,
               y = participant,
               color = rank)) +
  viridis::scale_color_viridis(option = "plasma", direction = -1, begin = 0.2, end = 0.8)
```
